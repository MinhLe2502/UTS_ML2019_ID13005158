{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinhLe2502/UTS_ML2019_ID13005158/blob/master/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9VvIoZuHTCW",
        "colab_type": "text"
      },
      "source": [
        "#***ID3 Decision Tree Algorithm***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzRWh6Xg5Nvp",
        "colab_type": "text"
      },
      "source": [
        "##**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luFkrg_N-ifK",
        "colab_type": "text"
      },
      "source": [
        "    ID3 decision tree is an algorithm invented by Ross Quinlan, and it is a tree in which each branch node represents a choice between several alternatives, and each leaf node represents a classification or decision. ID3 decision tree typically used in the machine learning and natural language processing domains.\n",
        "\n",
        "    In this report, we will use two different training sets as the input. Both training sets contain the same number of attributes and have the same name. However, one training set contains only categorical attributes; the other training set includes both quantitative and categorical attributes.\n",
        "\n",
        "    Through analyzing the training sets and through training the classifier to produce a decsion tree, then we need interpret the testing set to get the result, which is the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sovOeA05RpH",
        "colab_type": "text"
      },
      "source": [
        "##**Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IqUqJNO5UDS",
        "colab_type": "text"
      },
      "source": [
        "###**Challanges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubIRI_GcR90c",
        "colab_type": "text"
      },
      "source": [
        "    Decision tree is heavily depending on the source of information, and decision tree can not work well with super attributes. When working with super attributes, the algorithm may pick the super attributes as the root node, which may create a perfect classifier, but the algorithm will process ineffectively on unseen data.\n",
        "    \n",
        "    Decision tree is a sophisticated algorithm. To calculate the best split of data, the algorithm needs to compute the probabilities of different possibilities and then compute the weights and Gini impurity to create a decision tree. Also, a decision tree only can process one single data field at a time, which makes decision tree is time-consuming.\n",
        "    \n",
        "    Decision tree is also unproductive when working with quantitative super attributes variable. A decision tree can only work well with quantitative attributes with a small number of values. To optimize the decision tree with super quantitative attributes, it requires an extensive training set. \n",
        "\n",
        "    Decision tree is also limited in the analysis. Decision tree is incapable of predicting continuous values and estimate decision and values for continuous attributes. In addition, because decision tree can only process one single data filed at a time which can increase the possibility with duplicate sub-tree on different paths.\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaMDdrmh5Y8_",
        "colab_type": "text"
      },
      "source": [
        "###**Data Structures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iVxGYUYpAWC-"
      },
      "source": [
        "<img src=\"https://github.com/MinhLe2502/UTS_ML2019_ID13005158/raw/master/Training%20set%201%20model.jpg\" height=\"400\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVWPot1p0lzW",
        "colab_type": "text"
      },
      "source": [
        "$$Training\\ set\\ 1\\ Data\\ Structures$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sVN73ZKFAV2K"
      },
      "source": [
        "<img src=\"https://github.com/MinhLe2502/UTS_ML2019_ID13005158/raw/master/Training%20set%202%20model.jpg\" height=\"400\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDmZLYVw0orC",
        "colab_type": "text"
      },
      "source": [
        "$$Training\\ set\\ 2\\ Data\\ Structures$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEpVcimTvV2",
        "colab_type": "text"
      },
      "source": [
        "###**Implentation Plan**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_eud4q-T1vU",
        "colab_type": "text"
      },
      "source": [
        "    Decision Tree does not work well with quantitative attributes. To solve that problem, I define a range of quantitative values by categoricals values. In the training set, I convert the range of 0 - 5 as No to Exercises and No to Eat Fastfood. 6 - 10 as Yes to Exercises and Yes to Eat Fastfood.\n",
        "    \n",
        "    To build a decision tree, first, I have to partition the data. The best way to partition a data is to divide the data set base on its feature. So, I create a question class. The method is by iterate through the data set. It transfers the data which have the same feature through the match_rows with a match function and different feature to a mismatch_rows.\n",
        "    \n",
        "    Then I find the Gini impurity. Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labelled according to the class distribution in the dataset. After that, I find the info_gain, which is the amount of information gained about a random variable or signal from observing another random variable.\n",
        "    \n",
        "    Then I split the data into different parts by iterating over every feature or value and calculating the information gain. I partition the data by moving the match data points to match_rows and the other to mismatch_rows.\n",
        "    \n",
        "    I create a class called Leaf; this class hold number of times a feature appears in the rows from the training data that reach this leaf. And a class called Decision_Node, this class holds the question and to the two arrays match_rows and mismatch_rows\n",
        "    \n",
        "    After I have the leaf and the decision nodes, I build a tree using recursion. First, I split the data which has the feature of highest frequency then continue that step until no more question can be asked. When there is no more question, we partition the data set to match_rows and mismatch_rows. And apply these steps with the mismatch rows till no question can be asked."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xi9GRQiqZh3",
        "colab_type": "text"
      },
      "source": [
        "##**Methodology**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJKGsl3vqfn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eRfxx1RqiYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training dataset 1\n",
        "#First three column: Gender - Exercise - Eat Fastfood\n",
        "#Exercise in the first training set is Yes or No\n",
        "#Eat Fastfood in the first training set is Yes or No\n",
        "#Last column is determine is what type of Body Measurments\n",
        "training_data_1 = [\n",
        "    ['Male', 'Yes', 'No', 'Fit'],\n",
        "    ['Male', 'Yes', 'Yes', 'Average'],\n",
        "    ['Male', 'No', 'Yes', 'Unfit'],\n",
        "    ['Male', 'No', 'No', 'Average'],\n",
        "    ['Female', 'Yes', 'No', 'Fit'],\n",
        "    ['Female', 'Yes', 'Yes', 'Average'],\n",
        "    ['Female', 'No', 'Yes', 'Unfit'],\n",
        "    ['Female', 'No', 'No', 'Fit']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s9cJhyyHk76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training dataset 2\n",
        "#First three column: Gender - Exercise - Eat Fastfood\n",
        "#Exercise in the second training set is average hour per week\n",
        "#Eat Fastfood is the number of times in one week\n",
        "#Last column is determine is what type of Body Measurments\n",
        "training_data_2 = [\n",
        "    ['Male', 10, 2, 'Fit'],\n",
        "    ['Male', 6, 7, 'Average'],\n",
        "    ['Male', 0, 8, 'Unfit'],\n",
        "    ['Male', 0, 4, 'Average'],\n",
        "    ['Female', 10, 1, 'Fit'],\n",
        "    ['Female', 6, 8, 'Average'],\n",
        "    ['Female', 0, 9, 'Unfit'],\n",
        "    ['Female', 0, 2, 'Average']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_25BQ94vBeID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define header for each column\n",
        "header = ['Gender','Exercises','Eat FastFood','Body Measurments']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkV9rWQrN6nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_counts(rows):\n",
        "    #Counts the number of each type of example in a dataset.\n",
        "    #Create a dictionary to store each feature of each column\n",
        "    counts = {}\n",
        "    for row in rows:\n",
        "        # in our dataset format, the label is always the last column\n",
        "        label = row[-1]\n",
        "        if label not in counts:\n",
        "            counts[label] = 0\n",
        "        counts[label] = counts[label] + 1\n",
        "    return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0rzp8klUkvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Question:\n",
        "\n",
        "    def __init__(self, column, value):\n",
        "        self.column = column\n",
        "        self.value = value\n",
        "\n",
        "    def match(self, example):\n",
        "        # Compare the feature value in an example to the\n",
        "        # feature value in this question.\n",
        "        val = example[self.column]\n",
        "        if (isinstance(val, int) or isinstance(val, float)): #If the value is numeric\n",
        "          return val >= int(self.value)\n",
        "        else:\n",
        "          return val == self.value\n",
        "\n",
        "    def __repr__(self):\n",
        "        # This is just a helper method to print\n",
        "        # the question in a readable format.\n",
        "        condition = \"==\"\n",
        "        if isinstance(self.value, int) or isinstance(self.value, float): #If the value is numeric\n",
        "            condition = \">=\"\n",
        "        return \"Is %s %s %s?\" % (\n",
        "            header[self.column], condition, str(self.value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqrXCwAP23zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition(rows, question):\n",
        "    #this method is to partition the dataset, if the rows value match with the question value\n",
        "    #then add to match_rows array, otherwise, add to mismatch_rows\n",
        "    match_rows, mismatch_rows = [], []\n",
        "    for row in rows:\n",
        "        if question.match(row):\n",
        "            match_rows.append(row)\n",
        "        else:\n",
        "            mismatch_rows.append(row)\n",
        "    return match_rows, mismatch_rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uan2eryRSCu_",
        "colab_type": "text"
      },
      "source": [
        "**Gini Impurity**\n",
        "    \n",
        "    Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. Itâ€™s calculated as\n",
        "    \n",
        ">$G=\\sum_{i=1}^C p(i)*(1-p(i))$\n",
        "\n",
        "    Where C is the number of classes and p(i)p(i) is the probability of randomly picking an element of class ii.\n",
        "\n",
        "    When training a decision tree, the best split is chosen by maximizing the Gini Gain, which is calculated by subtracting the weighted impurities of the branches from the original impurity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YloJkwWIE6I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gini(rows):\n",
        "    #calculate gini impurity\n",
        "    counts = class_counts(rows)\n",
        "    impurity = 1\n",
        "    #itertaor through all labels \n",
        "    for labels in counts:\n",
        "        #Probabilty is the number of one type of label / sum of all labels\n",
        "        probablity = counts[labels] / float(len(rows))\n",
        "        #gini_impurity is by minus the intital impurity - probability**2\n",
        "        impurity = impurity - probablity**2\n",
        "    return impurity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_QY97LHPvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def info_gain(left, right, gini_impurity):\n",
        "    ##Subtracting the weighted impurities of the branches from the original impurity.\n",
        "    p = float(len(left)) / (len(left) + len(right))\n",
        "    return gini_impurity - p * gini(left) - (1 - p) * gini(right)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC9g6ilPHSAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_split(rows):\n",
        "    #By iterating through every feaure of of each column, we calculate the inforation gain\n",
        "    best_gain = 0         # temporary store the best info_gain\n",
        "    best_question = None  # temporary store the feature which relevant to info_gain\n",
        "    current_uncertainty = gini(rows)\n",
        "    n_features = len(rows[0]) - 1  # length of column\n",
        "\n",
        "    for col in range(n_features):\n",
        "        values = set([row[col] for row in rows])\n",
        "        for val in values:\n",
        "            question = Question(col, val)\n",
        "            match_rows, mismatch_rows = partition(rows, question)\n",
        "\n",
        "            # Skip this split if it doesn't divide the dataset.\n",
        "            if len(match_rows) == 0 or len(mismatch_rows) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate the information gain from this split\n",
        "            gain = info_gain(match_rows, mismatch_rows, current_uncertainty)\n",
        "\n",
        "            # You actually can use '>' instead of '>=' here\n",
        "            # but I wanted the tree to look a certain way for our\n",
        "            # toy dataset.\n",
        "            if gain > best_gain:\n",
        "                best_gain, best_question = gain, question\n",
        "\n",
        "    return best_gain, best_question"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UEt0LytHTQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Leaf:\n",
        "    #this class hold the the directory of each feature\n",
        "\n",
        "    def __init__(self, rows):\n",
        "        self.predictions = class_counts(rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNi4guonHUdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decision_Node:\n",
        "    #a decision nodes hold a referces to two child nodes\n",
        "\n",
        "    def __init__(self,\n",
        "                 question,\n",
        "                 match_branch,\n",
        "                 mismatch_branch):\n",
        "        self.question = question\n",
        "        self.match_branch = match_branch\n",
        "        self.mismatch_branch = mismatch_branch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDKolkM8HWIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tree(rows):\n",
        "    #devide the data set base on each unique values, calculate the info_gain \n",
        "    #and return the highest best_gain\n",
        "    gain, question = find_best_split(rows)\n",
        "\n",
        "    #if all features had been loop through, so no more gain then return a leaf\n",
        "    if gain == 0:\n",
        "        return Leaf(rows)\n",
        "\n",
        "    #By using question fuctions, we partition the data set based on which feature\n",
        "    #matches the questions.\n",
        "    match_rows, mismatch_rows = partition(rows, question)\n",
        "\n",
        "    # Recursively build the match branch.\n",
        "    match_branch = build_tree(match_rows)\n",
        "\n",
        "    # Recursively build the mismatch branch.\n",
        "    mismatch_branch = build_tree(mismatch_rows)\n",
        "\n",
        "    return Decision_Node(question, match_branch, mismatch_branch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYsF5stLHXe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_tree(node, spacing=\"\"):\n",
        "\n",
        "    # By partition and calculate the entrpopy, we use the feature has the most \n",
        "    #info_gain as the start node\n",
        "    if isinstance(node, Leaf):\n",
        "        print (spacing + \"Predict\", node.predictions)\n",
        "        return\n",
        "\n",
        "    # Print the question at this node\n",
        "    print (spacing + str(node.question))\n",
        "\n",
        "    # Call this function recursively on the match branch\n",
        "    print (spacing + '--> True:')\n",
        "    print_tree(node.match_branch, spacing + \"  \")\n",
        "\n",
        "    # Call this function recursively on the mismatch branch\n",
        "    print (spacing + '--> False:')\n",
        "    print_tree(node.mismatch_branch, spacing + \"  \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrVHV16hHYcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_tree_1 = build_tree(training_data_1)\n",
        "my_tree_2 = build_tree(training_data_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJxX3sjeHZUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "27436587-489b-4375-aa1d-7ccc4bb087f7"
      },
      "source": [
        "print_tree(my_tree_1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Eat FastFood == No?\n",
            "--> True:\n",
            "  Is Gender == Male?\n",
            "  --> True:\n",
            "    Is Exercises == No?\n",
            "    --> True:\n",
            "      Predict {'Average': 1}\n",
            "    --> False:\n",
            "      Predict {'Fit': 1}\n",
            "  --> False:\n",
            "    Predict {'Fit': 2}\n",
            "--> False:\n",
            "  Is Exercises == No?\n",
            "  --> True:\n",
            "    Predict {'Unfit': 2}\n",
            "  --> False:\n",
            "    Predict {'Average': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HMrfaOLKxw2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "9c1f3e83-7815-4304-fbfa-b2017c1500a0"
      },
      "source": [
        "print_tree(my_tree_2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Exercises >= 10?\n",
            "--> True:\n",
            "  Predict {'Fit': 2}\n",
            "--> False:\n",
            "  Is Eat FastFood >= 8?\n",
            "  --> True:\n",
            "    Is Exercises >= 6?\n",
            "    --> True:\n",
            "      Predict {'Average': 1}\n",
            "    --> False:\n",
            "      Predict {'Unfit': 2}\n",
            "  --> False:\n",
            "    Predict {'Average': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAH3enZlDJtD",
        "colab_type": "text"
      },
      "source": [
        "##**Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbm3RVo-F_aq",
        "colab_type": "text"
      },
      "source": [
        "###**Report execution on data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oOXinA5bzAa",
        "colab_type": "text"
      },
      "source": [
        "####Question "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr5dh1v-UsYv",
        "colab_type": "text"
      },
      "source": [
        "From the class Question, we can partition the data set by using the match method to compare the feature in the data set with the feaure in the question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il6FkCHZGCyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccc6472d-4d05-4885-a874-feffc2aa2196"
      },
      "source": [
        "#Demo\n",
        "#If the feature is a quantitative value\n",
        "Question (1,3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Is Exercises >= 3?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c89ssY_WV3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d9fa689-35cb-4305-e0ac-b2c6372f411f"
      },
      "source": [
        "#If the feature is a categorical value\n",
        "Question (1,'Yes')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Is Exercises == Yes?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC2f31wAb3Lk",
        "colab_type": "text"
      },
      "source": [
        "####Partitioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq5WZAgdXgZY",
        "colab_type": "text"
      },
      "source": [
        "Based on which feature of the data set match with the feature in the question, we can partition the data set in two parts, one side contains row which had the feature match with question, and the other part contain row which feature do not match with the question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGdPNZc9YQV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bbafd053-d22c-4bd4-9cb8-a6ed776ddf4a"
      },
      "source": [
        "#Demo\n",
        "#This partition divide data set whenther the rows contain Yes to going exercise\n",
        "match_rows, mismatch_rows = partition(training_data_1, Question(1,'Yes'))\n",
        "match_rows #this contain all rows which is yes to exercise"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'Yes', 'No', 'Fit'],\n",
              " ['Male', 'Yes', 'Yes', 'Average'],\n",
              " ['Female', 'Yes', 'No', 'Fit'],\n",
              " ['Female', 'Yes', 'Yes', 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYFBfHPa3v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "991f73e3-b245-4e67-f214-4ce5c61353b8"
      },
      "source": [
        "mismatch_rows #this contain all remianing rows"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'No', 'Yes', 'Unfit'],\n",
              " ['Male', 'No', 'No', 'Average'],\n",
              " ['Female', 'No', 'Yes', 'Unfit'],\n",
              " ['Female', 'No', 'No', 'Fit']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfO4KosabLs_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e98ae457-62ea-4cf7-f0a1-dcd756e77ba8"
      },
      "source": [
        "#Demo\n",
        "#This partition divide data set whenther the rows contain doing exercis >5 hours a week\n",
        "match_rows, mismatch_rows = partition(training_data_2, Question(1,5))\n",
        "match_rows #this contain all rows doing exercis >5 hours a week"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 10, 2, 'Fit'],\n",
              " ['Male', 6, 7, 'Average'],\n",
              " ['Female', 10, 1, 'Fit'],\n",
              " ['Female', 6, 8, 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9gajUi3bqLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "69f39739-b40d-497b-8a3d-84d0c496c0cf"
      },
      "source": [
        "mismatch_rows #this contain all remianing rows"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 0, 8, 'Unfit'],\n",
              " ['Male', 0, 4, 'Average'],\n",
              " ['Female', 0, 9, 'Unfit'],\n",
              " ['Female', 0, 2, 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzTYgXQcAPb",
        "colab_type": "text"
      },
      "source": [
        "####Gini Impurity and Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-35wvHE2cE1N",
        "colab_type": "text"
      },
      "source": [
        "Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "\n",
        "The data set has not been divided yet so we calculate gini impurity by using the whole data points.\n",
        "\n",
        "C is the number of class, p(1) is the probability of picking 'Fit', p(2) is the probability of picking 'Unfit', p(3) is the probability of picking 'Average', G is Gini Impurity.\n",
        "\n",
        "$C = 3$\n",
        "\n",
        "$p(1) = 2/8 = 0.25$\n",
        "\n",
        "$p(2) = 2/8 = 0.25$\n",
        "\n",
        "$p(3) = 4/8 = 0.5$\n",
        "\n",
        "$G= p(1)*(1-p(1))+p(2)*(1-p(2))+p(3)*(1-p(3))$\n",
        "\n",
        "$\\ \\ \\ = 0.25*(1-0.25)+0.25(1-0.25)+0.5*(1-0.5)$\n",
        "\n",
        "$\\ \\ \\ = 0.1875 + 0.1875+0.25$\n",
        "\n",
        "$\\ \\ \\ =0.625$\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jfS6LYwfX3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6cfb3d1-6922-433f-949a-40624b180883"
      },
      "source": [
        "#demo\n",
        "a = [['Fit'],\n",
        "    ['Average'],\n",
        "    ['Unfit'],\n",
        "    ['Average'],\n",
        "    ['Fit'],\n",
        "    ['Average'],\n",
        "    ['Unfit'],\n",
        "    ['Average']]\n",
        "\n",
        "gini(a)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIfR_tnZkZBC",
        "colab_type": "text"
      },
      "source": [
        "Information gain is calculated by taking the Gini impurity of the root minus the impurity of two child nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "051cb84e-74d5-4ff9-d68a-afb00538b531",
        "id": "XhqiJ8njrTyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Gini impurity of training data 1\n",
        "gini_impurity_1 = gini(training_data_1)\n",
        "gini_impurity_1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.65625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoeNjhJVsLwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b44f7ef-7138-4437-8b9a-a79423b1fba0"
      },
      "source": [
        "#Gini impurity of training data 2\n",
        "gini_impurity_2 = gini(training_data_2)\n",
        "gini_impurity_2"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIFAI4Ipkpvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eee315c0-6f82-4cfc-c360-aaaa822da8ec"
      },
      "source": [
        "#Partitioning on Yes to exercise of training set 1\n",
        "match_rows, mismatch_rows = partition(training_data_1, Question(1, 'Yes'))\n",
        "info_gain(match_rows, mismatch_rows, gini_impurity_1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50y_TIVltE79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5653a8f3-e9f2-4d80-dbd2-7dfa89c2370e"
      },
      "source": [
        "match_rows"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'Yes', 'No', 'Fit'],\n",
              " ['Male', 'Yes', 'Yes', 'Average'],\n",
              " ['Female', 'Yes', 'No', 'Fit'],\n",
              " ['Female', 'Yes', 'Yes', 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da4uBh0VtXzp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2a335ed1-1122-4b37-8447-d064bc0ebf09"
      },
      "source": [
        "mismatch_rows"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'No', 'Yes', 'Unfit'],\n",
              " ['Male', 'No', 'No', 'Average'],\n",
              " ['Female', 'No', 'Yes', 'Unfit'],\n",
              " ['Female', 'No', 'No', 'Fit']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvgllAEhq2ke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a30f2dd5-dd6d-4a4c-b6eb-be9bb817fa7b"
      },
      "source": [
        "#Partitioning on exercise >5 hours of training set 2\n",
        "match_rows, mismatch_rows = partition(training_data_2, Question(1, 7))\n",
        "info_gain(match_rows, mismatch_rows, gini_impurity_2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29166666666666663"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l57zA_9mtJSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0445f553-766e-4f7e-974e-b9bf6dad5b81"
      },
      "source": [
        "match_rows"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 10, 2, 'Fit'], ['Female', 10, 1, 'Fit']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmVSe-zMtb27",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "510e3dae-c7aa-435a-f87b-1394ce9a3ae4"
      },
      "source": [
        "mismatch_rows"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 6, 7, 'Average'],\n",
              " ['Male', 0, 8, 'Unfit'],\n",
              " ['Male', 0, 4, 'Average'],\n",
              " ['Female', 6, 8, 'Average'],\n",
              " ['Female', 0, 9, 'Unfit'],\n",
              " ['Female', 0, 2, 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9-X0p5lmet",
        "colab_type": "text"
      },
      "source": [
        "####Finding the best splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXmSQ_Vjlt_5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0c38ce3-62c7-4c0d-c43d-4184c30606f6"
      },
      "source": [
        "#Demo\n",
        "#find the best split using gini and info_gain to find the best feature to split\n",
        "#Training set 1 split\n",
        "best_gain, best_question = find_best_split(training_data_1)\n",
        "best_question"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Is Eat FastFood == No?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTj5Ap-ym4vQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07baff30-77d1-4954-d18c-77d4394f02ca"
      },
      "source": [
        "#Training set  split\n",
        "best_gain, best_question = find_best_split(training_data_2)\n",
        "best_question"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Is Exercises >= 10?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ckTUGZDMUw",
        "colab_type": "text"
      },
      "source": [
        "###**Perform and report testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATVSCxEcHad0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(row, node):\n",
        "    #This method is to classify the testing set feature\n",
        "    if isinstance(node, Leaf):\n",
        "        return node.predictions\n",
        "\n",
        "    #Compare the deature of the testing set with the feature of the tree\n",
        "    #To determin which road should continue\n",
        "    if node.question.match(row):\n",
        "        return classify(row, node.match_branch)\n",
        "    else:\n",
        "        return classify(row, node.mismatch_branch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZHgGwxIHcEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_leaf(counts):\n",
        "    total = sum(counts.values()) * 1.0\n",
        "    probs = {}\n",
        "    for labels in counts.keys():\n",
        "        probs[labels] = str(int(counts[labels] / total * 100)) + \"%\"\n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENBjC67_HhNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_data_1 = [\n",
        "    ['Male', 'No', 'No', 'Fit'],\n",
        "    ['Male', 'Yes', 'No', 'Average'],\n",
        "    ['Male', 'No', 'Yes', 'Unfit'],\n",
        "    ['Female', 'No', 'No', 'Fit'],\n",
        "    ['Female', 'Yes', 'No', 'Fit'],\n",
        "    ['Female', 'Yes', 'Yes', 'Average'],\n",
        "    ['Female', 'No', 'No', 'Unfit'],\n",
        "    ['Female', 'No', 'No', 'Fit']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Y16abWHiz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0dbabfd5-f93a-4271-bd76-0a8ff118ef6a"
      },
      "source": [
        "#Test data base on the training set 1\n",
        "for row in testing_data_1:\n",
        "    print (\"Actual: %s. Predicted: %s\" %\n",
        "           (row[-1], print_leaf(classify(row, my_tree_1))))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: Fit. Predicted: {'Average': '100%'}\n",
            "Actual: Average. Predicted: {'Fit': '100%'}\n",
            "Actual: Unfit. Predicted: {'Unfit': '100%'}\n",
            "Actual: Fit. Predicted: {'Fit': '100%'}\n",
            "Actual: Fit. Predicted: {'Fit': '100%'}\n",
            "Actual: Average. Predicted: {'Average': '100%'}\n",
            "Actual: Unfit. Predicted: {'Fit': '100%'}\n",
            "Actual: Fit. Predicted: {'Fit': '100%'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAN-9KQsCNLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_data_2 = [\n",
        "    ['Male', 0, 0, 'Fit'],\n",
        "    ['Male', 3, 4, 'Average'],\n",
        "    ['Male', 4, 8, 'Unfit'],\n",
        "    ['Male', 10, 9, 'Average'],\n",
        "    ['Female', 0, 0, 'Fit'],\n",
        "    ['Female', 9, 20, 'Average'],\n",
        "    ['Female', 6, 2, 'Unfit'],\n",
        "    ['Female', 4, 5, 'Fit']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXQU1OunQsya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "befb7d80-dcd2-4852-b63d-b5ffddfd6800"
      },
      "source": [
        "#Test data base on the training set 2\n",
        "for row in testing_data_2:\n",
        "    print (\"Actual: %s. Predicted: %s\" %\n",
        "           (row[-1], print_leaf(classify(row, my_tree_2))))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: Fit. Predicted: {'Average': '100%'}\n",
            "Actual: Average. Predicted: {'Average': '100%'}\n",
            "Actual: Unfit. Predicted: {'Unfit': '100%'}\n",
            "Actual: Average. Predicted: {'Fit': '100%'}\n",
            "Actual: Fit. Predicted: {'Average': '100%'}\n",
            "Actual: Average. Predicted: {'Average': '100%'}\n",
            "Actual: Unfit. Predicted: {'Average': '100%'}\n",
            "Actual: Fit. Predicted: {'Average': '100%'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1RWN3h0bKBl",
        "colab_type": "text"
      },
      "source": [
        "###**Perform Efficiency Analysis and Comparative Study**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aXzOnMgbNGL",
        "colab_type": "text"
      },
      "source": [
        "    In the training set 1, Exercises and Eat Fastfood are categorical attributes. Training set 2, Exercises and Eat Fastfood are quantitative attributes.\n",
        "    \n",
        "    So as I stated in Implenmentation plan, I convert the range of 0 - 5 as No to Exercises and No to Eat Fastfood. 6 - 10 as Yes to Exercises and Yes to Eat Fastfood. And make some condtion such as\n",
        "    \n",
        "> Exercises >5 and >5 Eat Fastfood: Average\n",
        "> Exercises <5 and <5 Eat Fastfood: Average\n",
        "> Exercises >5 and <5 Eat Fastfood: Fit\n",
        "> Exercises <5 and >5 Eat Fastfood: Unfit\n",
        "\n",
        "    Through testing with the classifier, we see that the classifier interpret the testing set and give out two results. The result of both have high accuracy, but by converted the quantitative value through categorical values, we can increase the accuracy.\n",
        "    \n",
        "    By using only categorical attributes, a decision tree can easily distinguish the feature to split the data and propose the decision. Using only categorical attributes can give high accuracy decision; however, in a large data set, not all value can be converted into categorical attributes. If all values are converted to categorical attributes, then the decision is biased and only accurate which the converted values but not with the original values.\n",
        "    \n",
        "    Using both categorical and quantitative values have lower accuracy than the training set with categorical attributes. When using quantitative attributes, the split is done with the elements higher than a threshold. At every split, the decision tree will take the best variable at that moment. However, with a super attribute, the decision tree needs to create many bins with requiring a lot of space which makes the program is time-consuming, and the accuracy is low due to many bins. \n",
        "    \n",
        "    In conclusion, although both have high accuracy. But the result is to explain how the classifier works. Because the data set is small and bias, which makes the efficiency is high, but the result is not accurate. To make the classifier more effectively, it requires an extensive data set with randomly chosen data points.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKgCgHYNGS2b",
        "colab_type": "text"
      },
      "source": [
        "##**Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHBamj7BHVtf",
        "colab_type": "text"
      },
      "source": [
        "    Through the report, we can see that the decision tree has many advantages and disadvantages. A decision tree size is small, so the reader can comfortable comprehend what data store in the tree and its purpose. Also, a decision tree can help the user to make a decision based on the training set. However, the decision tree requires an extensive training set to evaluate the testing fully. In conclusion, although the decision tree has high human cognition and aesthetic rules but requires high space utilisation and high time consumption.\n",
        "    \n",
        "    To improve the accuracy of the decision tree, we can use induction by feature pre-selection. By filter irrelevant or correlated feature, we may increase the efficiency of the decision tree by upgrading its performance. Also, small features can reduce complexity and make classification models interpret the data more efficiently. This process can be done by focusing on one subset of the relevant feature while ignore the other ones, by ignoring other subsets, the classifier can concentrate its attention to the main feature its need to interpret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIyTHqH5GVOT",
        "colab_type": "text"
      },
      "source": [
        "##**Ethical**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Suv3BpfgbM",
        "colab_type": "text"
      },
      "source": [
        "    Decision Tree had been applied in many different areas to make legal predictions in political and social sciences. One example of a decision tree to predict political sciences was using a boosted decision tree to predict the supreme court. The court's decisions may change based on the given information and unknown factors. However, the decision tree can predict the outcome by reducing the number of irrelevant variables and render then to make them practical in a prediction.\n",
        "      \n",
        "     Although, by using a decision tree, the court can make judgment more accurately, and the judges don't affect by unknown factors. However, using the decision tree is a threat to human causes, a decision tree can manipulate and modify the human way of thinking, which classify unethical decision tree practices. For example, when a court makes a judgment, they judge based on the circumstances, what crime did they commit and why they did it, however, a decision tree may filter some variables, and make a judgment without going through all features, and if the court abuse using  \n",
        "     \n",
        "     Also, a decision tree may use past data to make a judgment. With a decision tree, helps us in review old cases, provide risk profiles and assist us with legal matters. A decision tree can give us decisions through skewed data, false logic and biases. For example, based on a study in 2006 by Human Rights Data, they created an algorithm to the predicted likelihood of crime taking place in the US. However, the result was unfairly targeted black and Hispanic neighbourhoods. \n",
        "     \n",
        "     In conclusion, Decision Tree can help us make decisions by irrelevant filter information and give us the most accurate choice. However, decision tree also used past data to make judgments, so decision also changes the way of human thinking and may provide unfair judgment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBDFu-9GXeb",
        "colab_type": "text"
      },
      "source": [
        "##**Video Pitch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVGY4fNi01c",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tck3aPbpi1PB",
        "colab_type": "text"
      },
      "source": [
        "##**Appendix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tfa2PfyLYc",
        "colab_type": "text"
      },
      "source": [
        "###References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdhwngaOi1ia",
        "colab_type": "text"
      },
      "source": [
        "Kunal, C. 'A-Z of Decision Trees', viewed 21 September 2019,  <https://medium.com/analytics-vidhya/part-2-a-z-decision-trees-f88d704968d1>\n",
        "\n",
        "Obaidul, C., 'The Ethical Implications in the Application of Artificial Intelligence in Law', The Startup, viewed 20 September 2019, <https://medium.com/swlh/the-ethical-implications-in-the-application-of-artificial-intelligence-in-law-9906c16e3fa8>\n",
        "\n",
        "\n",
        "Perner, P., 'Improving The Accuracy of Decision Tree Induction by Feature Pre-Selection', Aplied Artifical Intelligent 2001, vol. 15, no. 8, pp. 747 - 760, viewed 21 September 2019,  <http://www.ibai-institut.de/files/aai01.pdf> \n",
        "\n",
        "Zhixiao, A. 'Variable Reduction in SAS by Using Weight of Evidence and Information Value', SAS Global Forum 2013, viewed 23 September 2019, <http://support.sas.com/resources/papers/proceedings13/095-2013.pdf>\n",
        "\n",
        "Zhou, V. 'A Simple Explanation of Gini Impurity', viewed 21 September 2019,  <https://victorzhou.com/blog/gini-impurity/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd28tACSyNbd",
        "colab_type": "text"
      },
      "source": [
        "###Link to Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co0Xd3LsyQSZ",
        "colab_type": "text"
      },
      "source": [
        "<https://colab.research.google.com/drive/1KvrYfOKdCXMV2JZKw_x9O_qgTc-3xYPb?authuser=3#scrollTo=Pd28tACSyNbd>"
      ]
    }
  ]
}