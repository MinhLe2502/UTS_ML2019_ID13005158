{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinhLe2502/UTS_ML2019_ID13005158/blob/master/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9VvIoZuHTCW",
        "colab_type": "text"
      },
      "source": [
        "#***ID3 Decision Tree Algorithm***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzRWh6Xg5Nvp",
        "colab_type": "text"
      },
      "source": [
        "##**Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luFkrg_N-ifK",
        "colab_type": "text"
      },
      "source": [
        "    ID3 decision tree is an algorithm invented by Ross Quinlan, and it is a tree in which each branch node represents a choice between several alternatives, and each leaf node represents a classification or decision. ID3 decision tree typically used in the machine learning and natural language processing domains.\n",
        "\n",
        "    In this report, we will use two different training set as the input. Both training set contains the same number of attributes and have the same name. However, one training set contains only categorical attributes; the other training set includes both quantitative and categorical attributes.\n",
        "\n",
        "    By analyze the training sets and train out classifier to produce a decsion tree, we can generate the output is the result by interpret the testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sovOeA05RpH",
        "colab_type": "text"
      },
      "source": [
        "##**Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IqUqJNO5UDS",
        "colab_type": "text"
      },
      "source": [
        "###**Challanges**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubIRI_GcR90c",
        "colab_type": "text"
      },
      "source": [
        "    Decision tree is heavily depending on the source of information, and decision tree can not work well with super attributes. When working with super attributes, the algorithm may pick the super attributes as the root node, which may create a perfect classifier, but the algorithm will process ineffectively on unseen data.\n",
        "    \n",
        "    Decision tree is a sophisticated algorithm. To calculate the best split of data, the algorithm needs to compute the probabilities of different possibilities and then compute the weights and Gini impurity to create a decision tree. Also, a decision tree only can process one single data field at a time, which makes decision tree is time-consuming.\n",
        "    \n",
        "    Decision tree is also unproductive when working with quantitative super attributes variable. A decision tree can only work well with quantitative attributes with a small number of values. To optimize the decision tree with super quantitative attributes, it requires an extensive training set. \n",
        "\n",
        "    Decision tree is also limited in the analysis. Decision tree is incapable of predicting continuous values and estimate decision and values for continuous attributes. In addition, because decision tree can only process one single data filed at a time which can increase the possibility with duplicate sub-tree on different paths.\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaMDdrmh5Y8_",
        "colab_type": "text"
      },
      "source": [
        "###**Data Structures**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iVxGYUYpAWC-"
      },
      "source": [
        "<img src=\"https://github.com/MinhLe2502/UTS_ML2019_ID13005158/raw/master/Training%20set%201%20model.jpg\" height=\"400\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVWPot1p0lzW",
        "colab_type": "text"
      },
      "source": [
        "$$Training\\ set\\ 1\\ Data\\ Structures$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sVN73ZKFAV2K"
      },
      "source": [
        "<img src=\"https://github.com/MinhLe2502/UTS_ML2019_ID13005158/raw/master/Training%20set%202%20model.jpg\" height=\"400\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDmZLYVw0orC",
        "colab_type": "text"
      },
      "source": [
        "$$Training\\ set\\ 2\\ Data\\ Structures$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEpVcimTvV2",
        "colab_type": "text"
      },
      "source": [
        "###**Implentation Plan**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_eud4q-T1vU",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xi9GRQiqZh3",
        "colab_type": "text"
      },
      "source": [
        "##**Methodology**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJKGsl3vqfn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eRfxx1RqiYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training dataset 1\n",
        "#First three column: Gender - Exercise - Eat Fastfood\n",
        "#Exercise in the first training set is Yes or No\n",
        "#Eat Fastfood in the first training set is Yes or No\n",
        "#Last column is determine is what type of Body Measurments\n",
        "training_data_1 = [\n",
        "    ['Male', 'Yes', 'No', 'Fit'],\n",
        "    ['Male', 'Yes', 'Yes', 'Average'],\n",
        "    ['Male', 'No', 'Yes', 'Unfit'],\n",
        "    ['Male', 'No', 'No', 'Average'],\n",
        "    ['Female', 'Yes', 'No', 'Fit'],\n",
        "    ['Female', 'Yes', 'Yes', 'Average'],\n",
        "    ['Female', 'No', 'Yes', 'Unfit'],\n",
        "    ['Female', 'No', 'No', 'Fit']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s9cJhyyHk76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training dataset 2\n",
        "#First three column: Gender - Exercise - Eat Fastfood\n",
        "#Exercise in the second training set is average hour per week\n",
        "#Eat Fastfood is the number of times in one week\n",
        "#Last column is determine is what type of Body Measurments\n",
        "training_data_2 = [\n",
        "    ['Male', 10, 2, 'Fit'],\n",
        "    ['Male', 6, 7, 'Average'],\n",
        "    ['Male', 0, 8, 'Unfit'],\n",
        "    ['Male', 0, 4, 'Average'],\n",
        "    ['Female', 10, 1, 'Fit'],\n",
        "    ['Female', 6, 8, 'Average'],\n",
        "    ['Female', 0, 9, 'Unfit'],\n",
        "    ['Female', 0, 2, 'Average']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_25BQ94vBeID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define header for each column\n",
        "header = ['Gender','Exercises','Eat FastFood','Body Measurments']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkV9rWQrN6nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_counts(rows):\n",
        "    #Counts the number of each type of example in a dataset.\n",
        "    #Create a dictionary to store each feature of each column\n",
        "    counts = {}\n",
        "    for row in rows:\n",
        "        # in our dataset format, the label is always the last column\n",
        "        label = row[-1]\n",
        "        if label not in counts:\n",
        "            counts[label] = 0\n",
        "        counts[label] = counts[label] + 1\n",
        "    return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0rzp8klUkvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Question:\n",
        "\n",
        "    def __init__(self, column, value):\n",
        "        self.column = column\n",
        "        self.value = value\n",
        "\n",
        "    def match(self, example):\n",
        "        # Compare the feature value in an example to the\n",
        "        # feature value in this question.\n",
        "        val = example[self.column]\n",
        "        if (isinstance(val, int) or isinstance(val, float)): #If the value is numeric\n",
        "          return val >= int(self.value)\n",
        "        else:\n",
        "          return val == self.value\n",
        "\n",
        "    def __repr__(self):\n",
        "        # This is just a helper method to print\n",
        "        # the question in a readable format.\n",
        "        condition = \"==\"\n",
        "        if isinstance(self.value, int) or isinstance(self.value, float): #If the value is numeric\n",
        "            condition = \">=\"\n",
        "        return \"Is %s %s %s?\" % (\n",
        "            header[self.column], condition, str(self.value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqrXCwAP23zP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def partition(rows, question):\n",
        "    #this method is to partition the dataset, if the rows value match with the question value\n",
        "    #then add to match_rows array, otherwise, add to mismatch_rows\n",
        "    match_rows, mismatch_rows = [], []\n",
        "    for row in rows:\n",
        "        if question.match(row):\n",
        "            match_rows.append(row)\n",
        "        else:\n",
        "            mismatch_rows.append(row)\n",
        "    return match_rows, mismatch_rows"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uan2eryRSCu_",
        "colab_type": "text"
      },
      "source": [
        "**Gini Impurity**\n",
        "    \n",
        "    Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. It’s calculated as\n",
        "    \n",
        ">$G=\\sum_{i=1}^C p(i)*(1-p(i))$\n",
        "\n",
        "    Where C is the number of classes and p(i)p(i) is the probability of randomly picking an element of class ii.\n",
        "\n",
        "    When training a decision tree, the best split is chosen by maximizing the Gini Gain, which is calculated by subtracting the weighted impurities of the branches from the original impurity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YloJkwWIE6I5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gini(rows):\n",
        "    #calculate gini impurity\n",
        "    counts = class_counts(rows)\n",
        "    impurity = 1\n",
        "    for lbl in counts:\n",
        "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
        "        impurity -= prob_of_lbl**2\n",
        "    return impurity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG_QY97LHPvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def info_gain(left, right, gini_impurity):\n",
        "    ##Subtracting the weighted impurities of the branches from the original impurity.\n",
        "    p = float(len(left)) / (len(left) + len(right))\n",
        "    return gini_impurity - p * gini(left) - (1 - p) * gini(right)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC9g6ilPHSAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_split(rows):\n",
        "    #By iterating through every feaure of of each column, we calculate the inforation gain\n",
        "    best_gain = 0         # temporary store the best info_gain\n",
        "    best_question = None  # temporary store the feature which relevant to info_gain\n",
        "    current_uncertainty = gini(rows)\n",
        "    n_features = len(rows[0]) - 1  # number of columns\n",
        "\n",
        "    for col in range(n_features):\n",
        "        values = set([row[col] for row in rows])\n",
        "        for val in values:\n",
        "            question = Question(col, val)\n",
        "            match_rows, mismatch_rows = partition(rows, question)\n",
        "\n",
        "            # Skip this split if it doesn't divide the dataset.\n",
        "            if len(match_rows) == 0 or len(mismatch_rows) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate the information gain from this split\n",
        "            gain = info_gain(match_rows, mismatch_rows, current_uncertainty)\n",
        "\n",
        "            # You actually can use '>' instead of '>=' here\n",
        "            # but I wanted the tree to look a certain way for our\n",
        "            # toy dataset.\n",
        "            if gain > best_gain:\n",
        "                best_gain, best_question = gain, question\n",
        "\n",
        "    return best_gain, best_question"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UEt0LytHTQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Leaf:\n",
        "    #this class hold the the directory of each feature\n",
        "\n",
        "    def __init__(self, rows):\n",
        "        self.predictions = class_counts(rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNi4guonHUdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decision_Node:\n",
        "    #a decision nodes hold a referces to two child nodes\n",
        "\n",
        "    def __init__(self,\n",
        "                 question,\n",
        "                 match_branch,\n",
        "                 mismatch_branch):\n",
        "        self.question = question\n",
        "        self.match_branch = match_branch\n",
        "        self.mismatch_branch = mismatch_branch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDKolkM8HWIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_tree(rows):\n",
        "    #devide the data set base on each unique values, calculate the info_gain \n",
        "    #and return the highest best_gain\n",
        "    gain, question = find_best_split(rows)\n",
        "\n",
        "    #if all features had been loop through, so no more gain then return a leaf\n",
        "    if gain == 0:\n",
        "        return Leaf(rows)\n",
        "\n",
        "    #By using question fuctions, we partition the data set based on which feature\n",
        "    #matches the questions.\n",
        "    match_rows, mismatch_rows = partition(rows, question)\n",
        "\n",
        "    # Recursively build the match branch.\n",
        "    match_branch = build_tree(match_rows)\n",
        "\n",
        "    # Recursively build the mismatch branch.\n",
        "    mismatch_branch = build_tree(mismatch_rows)\n",
        "\n",
        "    return Decision_Node(question, match_branch, mismatch_branch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYsF5stLHXe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_tree(node, spacing=\"\"):\n",
        "\n",
        "    # By partition and calculate the entrpopy, we use the feature has the most \n",
        "    #info_gain as the start node\n",
        "    if isinstance(node, Leaf):\n",
        "        print (spacing + \"Predict\", node.predictions)\n",
        "        return\n",
        "\n",
        "    # Print the question at this node\n",
        "    print (spacing + str(node.question))\n",
        "\n",
        "    # Call this function recursively on the match branch\n",
        "    print (spacing + '--> True:')\n",
        "    print_tree(node.match_branch, spacing + \"  \")\n",
        "\n",
        "    # Call this function recursively on the mismatch branch\n",
        "    print (spacing + '--> False:')\n",
        "    print_tree(node.mismatch_branch, spacing + \"  \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrVHV16hHYcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_tree_1 = build_tree(training_data_1)\n",
        "my_tree_2 = build_tree(training_data_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJxX3sjeHZUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f0ff8a5b-6448-4b5c-9f33-be250516c7c4"
      },
      "source": [
        "print_tree(my_tree_1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Eat FastFood == No?\n",
            "--> True:\n",
            "  Is Gender == Male?\n",
            "  --> True:\n",
            "    Is Exercises == No?\n",
            "    --> True:\n",
            "      Predict {'Average': 1}\n",
            "    --> False:\n",
            "      Predict {'Fit': 1}\n",
            "  --> False:\n",
            "    Predict {'Fit': 2}\n",
            "--> False:\n",
            "  Is Exercises == No?\n",
            "  --> True:\n",
            "    Predict {'Unfit': 2}\n",
            "  --> False:\n",
            "    Predict {'Average': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HMrfaOLKxw2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "727a05f4-7ae1-4fd4-d217-00f8b07db4bf"
      },
      "source": [
        "print_tree(my_tree_2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Exercises >= 10?\n",
            "--> True:\n",
            "  Predict {'Fit': 2}\n",
            "--> False:\n",
            "  Is Eat FastFood >= 8?\n",
            "  --> True:\n",
            "    Is Exercises >= 6?\n",
            "    --> True:\n",
            "      Predict {'Average': 1}\n",
            "    --> False:\n",
            "      Predict {'Unfit': 2}\n",
            "  --> False:\n",
            "    Predict {'Average': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAH3enZlDJtD",
        "colab_type": "text"
      },
      "source": [
        "##**Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbm3RVo-F_aq",
        "colab_type": "text"
      },
      "source": [
        "###**Report execution on data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oOXinA5bzAa",
        "colab_type": "text"
      },
      "source": [
        "####Question "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr5dh1v-UsYv",
        "colab_type": "text"
      },
      "source": [
        "From the class Question, we can partition the data set by using the match method to compare the feature in the data set with the feaure in the question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il6FkCHZGCyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2838c4ba-c5c4-42e6-b1d2-6b3d8562d841"
      },
      "source": [
        "#Demo\n",
        "#If the feature is a quantitative value\n",
        "Question (1,3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Is Exercises >= 3?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c89ssY_WV3f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25a35623-d2fb-4262-cf0f-6db6a798f504"
      },
      "source": [
        "#If the feature is a categorical value\n",
        "Question (1,'Yes')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Is Exercises == Yes?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC2f31wAb3Lk",
        "colab_type": "text"
      },
      "source": [
        "####Partitioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq5WZAgdXgZY",
        "colab_type": "text"
      },
      "source": [
        "Based on which feature of the data set match with the feature in the question, we can partition the data set in two parts, one side contains row which had the feature match with question, and the other part contain row which feature do not match with the question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGdPNZc9YQV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "cdc2f939-7f9c-4c25-cbdd-6031582c1c32"
      },
      "source": [
        "#Demo\n",
        "#This partition divide data set whenther the rows contain Yes to going exercise\n",
        "match_rows, mismatch_rows = partition(training_data_1, Question(1,'Yes'))\n",
        "match_rows #this contain all rows which is yes to exercise"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'Yes', 'No', 'Fit'],\n",
              " ['Male', 'Yes', 'Yes', 'Average'],\n",
              " ['Female', 'Yes', 'No', 'Fit'],\n",
              " ['Female', 'Yes', 'Yes', 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSYFBfHPa3v9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "83d14c84-6fa5-420b-8180-805d206b0f61"
      },
      "source": [
        "mismatch_rows #this contain all remianing rows"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'No', 'Yes', 'Unfit'],\n",
              " ['Male', 'No', 'No', 'Average'],\n",
              " ['Female', 'No', 'Yes', 'Unfit'],\n",
              " ['Female', 'No', 'No', 'Fit']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfO4KosabLs_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "43e21ec1-04f3-4c81-80b9-9389fa93769e"
      },
      "source": [
        "#Demo\n",
        "#This partition divide data set whenther the rows contain doing exercis >5 hours a week\n",
        "match_rows, mismatch_rows = partition(training_data_2, Question(1,5))\n",
        "match_rows #this contain all rows doing exercis >5 hours a week"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 10, 2, 'Fit'],\n",
              " ['Male', 6, 7, 'Average'],\n",
              " ['Female', 10, 1, 'Fit'],\n",
              " ['Female', 6, 8, 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9gajUi3bqLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e269e668-e898-4dfb-eaa7-6211b718d204"
      },
      "source": [
        "mismatch_rows #this contain all remianing rows"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 0, 8, 'Unfit'],\n",
              " ['Male', 0, 4, 'Average'],\n",
              " ['Female', 0, 9, 'Unfit'],\n",
              " ['Female', 0, 2, 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wzTYgXQcAPb",
        "colab_type": "text"
      },
      "source": [
        "####Gini Impurity and Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-35wvHE2cE1N",
        "colab_type": "text"
      },
      "source": [
        "Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset.\n",
        "\n",
        "The data set has not been divided yet so we calculate gini impurity by using the whole data points.\n",
        "\n",
        "C is the number of class, p(1) is the probability of picking 'Fit', p(2) is the probability of picking 'Unfit', p(3) is the probability of picking 'Average', G is Gini Impurity.\n",
        "\n",
        "$C = 3$\n",
        "\n",
        "$p(1) = 2/8 = 0.25$\n",
        "\n",
        "$p(2) = 2/8 = 0.25$\n",
        "\n",
        "$p(3) = 4/8 = 0.5$\n",
        "\n",
        "$G= p(1)*(1-p(1))+p(2)*(1-p(2))+p(3)*(1-p(3))$\n",
        "\n",
        "$\\ \\ \\ = 0.25*(1-0.25)+0.25(1-0.25)+0.5*(1-0.5)$\n",
        "\n",
        "$\\ \\ \\ = 0.1875 + 0.1875+0.25$\n",
        "\n",
        "$\\ \\ \\ =0.625$\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jfS6LYwfX3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37045e26-7ba9-46af-8b5d-aa6ea93d7828"
      },
      "source": [
        "#demo\n",
        "a = [['Fit'],\n",
        "    ['Average'],\n",
        "    ['Unfit'],\n",
        "    ['Average'],\n",
        "    ['Fit'],\n",
        "    ['Average'],\n",
        "    ['Unfit'],\n",
        "    ['Average']]\n",
        "\n",
        "gini(a)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIfR_tnZkZBC",
        "colab_type": "text"
      },
      "source": [
        "Information gain is calculated by taking the Gini impurity of the root minus the impurity of two child nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "39dd5629-278f-4cad-e6c2-dd986781b49f",
        "id": "XhqiJ8njrTyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Gini impurity of training data 1\n",
        "gini_impurity_1 = gini(training_data_1)\n",
        "gini_impurity_1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.65625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoeNjhJVsLwL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c70de32b-d5da-48c6-af40-b6e6b8af7606"
      },
      "source": [
        "#Gini impurity of training data 2\n",
        "gini_impurity_2 = gini(training_data_2)\n",
        "gini_impurity_2"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIFAI4Ipkpvs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bc9ee02-09ae-4de9-a766-9c2ce49df9a0"
      },
      "source": [
        "#Partitioning on Yes to exercise of training set 1\n",
        "match_rows, mismatch_rows = partition(training_data_1, Question(1, 'Yes'))\n",
        "info_gain(match_rows, mismatch_rows, gini_impurity_1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50y_TIVltE79",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "29af0560-6c26-481d-f81c-539d2cd9a228"
      },
      "source": [
        "match_rows"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'Yes', 'No', 'Fit'],\n",
              " ['Male', 'Yes', 'Yes', 'Average'],\n",
              " ['Female', 'Yes', 'No', 'Fit'],\n",
              " ['Female', 'Yes', 'Yes', 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da4uBh0VtXzp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "470d5b12-59dc-4281-bb63-16e5fecf49b5"
      },
      "source": [
        "mismatch_rows"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 'No', 'Yes', 'Unfit'],\n",
              " ['Male', 'No', 'No', 'Average'],\n",
              " ['Female', 'No', 'Yes', 'Unfit'],\n",
              " ['Female', 'No', 'No', 'Fit']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvgllAEhq2ke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2309dc78-991b-4cc1-8260-801178904afa"
      },
      "source": [
        "#Partitioning on exercise >5 hours of training set 2\n",
        "match_rows, mismatch_rows = partition(training_data_2, Question(1, 7))\n",
        "info_gain(match_rows, mismatch_rows, gini_impurity_2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29166666666666663"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l57zA_9mtJSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27e13ecd-0d62-4593-b20a-5c804f7ea2d8"
      },
      "source": [
        "match_rows"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 10, 2, 'Fit'], ['Female', 10, 1, 'Fit']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmVSe-zMtb27",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6e5cbcf2-b7e5-419f-a38d-e0ec51e27627"
      },
      "source": [
        "mismatch_rows"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Male', 6, 7, 'Average'],\n",
              " ['Male', 0, 8, 'Unfit'],\n",
              " ['Male', 0, 4, 'Average'],\n",
              " ['Female', 6, 8, 'Average'],\n",
              " ['Female', 0, 9, 'Unfit'],\n",
              " ['Female', 0, 2, 'Average']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2ckTUGZDMUw",
        "colab_type": "text"
      },
      "source": [
        "###**Perform and report testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATVSCxEcHad0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(row, node):\n",
        "    #This method is to classify the testing set feature\n",
        "    if isinstance(node, Leaf):\n",
        "        return node.predictions\n",
        "\n",
        "    #Compare the deature of the testing set with the feature of the tree\n",
        "    #To determin which road should continue\n",
        "    if node.question.match(row):\n",
        "        return classify(row, node.match_branch)\n",
        "    else:\n",
        "        return classify(row, node.mismatch_branch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZHgGwxIHcEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_leaf(counts):\n",
        "    total = sum(counts.values()) * 1.0\n",
        "    probs = {}\n",
        "    for lbl in counts.keys():\n",
        "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENBjC67_HhNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_data_1 = [\n",
        "    ['Male', 'No', 'No', 'Fit'],\n",
        "    ['Male', 'Yes', 'No', 'Average'],\n",
        "    ['Male', 'No', 'Yes', 'Unfit'],\n",
        "    ['Female', 'No', 'No', 'Fit'],\n",
        "    ['Female', 'Yes', 'No', 'Fit'],\n",
        "    ['Female', 'Yes', 'Yes', 'Average'],\n",
        "    ['Female', 'No', 'No', 'Unfit'],\n",
        "    ['Female', 'No', 'No', 'Fit']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Y16abWHiz0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4fb9576c-ea6e-4aa3-d44c-ee579b2b6af6"
      },
      "source": [
        "#Test data base on the training set 1\n",
        "for row in testing_data_1:\n",
        "    print (\"Actual: %s. Predicted: %s\" %\n",
        "           (row[-1], print_leaf(classify(row, my_tree_1))))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: Fit. Predicted: {'Average': '100%'}\n",
            "Actual: Average. Predicted: {'Fit': '100%'}\n",
            "Actual: Unfit. Predicted: {'Unfit': '100%'}\n",
            "Actual: Fit. Predicted: {'Fit': '100%'}\n",
            "Actual: Fit. Predicted: {'Fit': '100%'}\n",
            "Actual: Average. Predicted: {'Average': '100%'}\n",
            "Actual: Unfit. Predicted: {'Fit': '100%'}\n",
            "Actual: Fit. Predicted: {'Fit': '100%'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSyfE5OkwP-V",
        "colab_type": "text"
      },
      "source": [
        "> >Testing in the testing set 1 throgh training set 1, although changing the labels, but we can see the accuracy is 100%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAN-9KQsCNLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing_data_2 = [\n",
        "    ['Male', 0, 0, 'Fit'],\n",
        "    ['Male', 3, 4, 'Average'],\n",
        "    ['Male', 4, 8, 'Unfit'],\n",
        "    ['Male', 10, 9, 'Average'],\n",
        "    ['Female', 0, 0, 'Fit'],\n",
        "    ['Female', 9, 20, 'Average'],\n",
        "    ['Female', 6, 2, 'Unfit'],\n",
        "    ['Female', 4, 5, 'Fit']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXQU1OunQsya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5442305a-5534-412c-f1a3-6c849212a71c"
      },
      "source": [
        "#Test data base on the training set 2\n",
        "for row in testing_data_2:\n",
        "    print (\"Actual: %s. Predicted: %s\" %\n",
        "           (row[-1], print_leaf(classify(row, my_tree_2))))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual: Fit. Predicted: {'Average': '100%'}\n",
            "Actual: Average. Predicted: {'Average': '100%'}\n",
            "Actual: Unfit. Predicted: {'Unfit': '100%'}\n",
            "Actual: Average. Predicted: {'Fit': '100%'}\n",
            "Actual: Fit. Predicted: {'Average': '100%'}\n",
            "Actual: Average. Predicted: {'Average': '100%'}\n",
            "Actual: Unfit. Predicted: {'Average': '100%'}\n",
            "Actual: Fit. Predicted: {'Average': '100%'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKgCgHYNGS2b",
        "colab_type": "text"
      },
      "source": [
        "##**Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHBamj7BHVtf",
        "colab_type": "text"
      },
      "source": [
        "    Through the report, we can see that the decision tree has many advantages and disadvantages. A decision tree size is small, so the reader can comfortable comprehend what data store in the tree and its purpose. Also, a decision tree can help the user to make a decision based on the training set. However, the decision tree requires an extensive training set to evaluate the testing fully. In conclusion, although the decision tree has high human cognition and aesthetic rules but requires high space utilisation and high time consumption.\n",
        "    \n",
        "    To improve the accuracy of the decision tree, we can use induction by feature pre-selection. By filter irrelevant or correlated feature, we may increase the efficiency of the decision tree by upgrading its performance. Also, small features can reduce complexity and make classification models interpret the data more efficiently. This process can be done by focusing on one subset of the relevant feature while ignore the other ones, by ignoring other subsets, the classifier can concentrate its attention to the main feature its need to interpret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIyTHqH5GVOT",
        "colab_type": "text"
      },
      "source": [
        "##**Ethical**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Suv3BpfgbM",
        "colab_type": "text"
      },
      "source": [
        "    Decision Tree had been applied in many different areas to make legal predictions in political and social sciences. One example of a decision tree to predict political sciences was using a boosted decision tree to predict the supreme court. The court's decisions may change based on the given information and unknown factors. However, the decision tree can predict the outcome by reducing the number of irrelevant variables and render then to make them practical in a prediction.\n",
        "      \n",
        "     Although, by using a decision tree, the court can make judgment more accurately, and the judges don't affect by unknown factors. However, using the decision tree is a threat to human causes, a decision tree can manipulate and modify the human way of thinking, which classify unethical decision tree practices. For example, when a court makes a judgment, they judge based on the circumstances, what crime did they commit and why they did it, however, a decision tree may filter some variables, and make a judgment without going through all features, and if the court abuse using  \n",
        "     \n",
        "     Also, a decision tree may use past data to make a judgment. With a decision tree, helps us in review old cases, provide risk profiles and assist us with legal matters. A decision tree can give us decisions through skewed data, false logic and biases. For example, based on a study in 2006 by Human Rights Data, they created an algorithm to the predicted likelihood of crime taking place in the US. However, the result was unfairly targeted black and Hispanic neighbourhoods. \n",
        "     \n",
        "     In conclusion, Decision Tree can help us make decisions by irrelevant filter information and give us the most accurate choice. However, decision tree also used past data to make judgments, so decision also changes the way of human thinking and may provide unfair judgment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiBDFu-9GXeb",
        "colab_type": "text"
      },
      "source": [
        "##**Video Pitch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVGY4fNi01c",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tck3aPbpi1PB",
        "colab_type": "text"
      },
      "source": [
        "##**Appendix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdhwngaOi1ia",
        "colab_type": "text"
      },
      "source": [
        "http://www.ibai-institut.de/files/aai01.pdf\n",
        "\n",
        "https://medium.com/swlh/the-ethical-implications-in-the-application-of-artificial-intelligence-in-law-9906c16e3fa8\n",
        "\n",
        "https://ro.uow.edu.au/cgi/viewcontent.cgi?article=2603&context=commpapers"
      ]
    }
  ]
}